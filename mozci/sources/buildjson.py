#!/usr/bin/env python
"""
This module helps with the buildjson data generated by the Release Engineering
systems: http://builddata.pub.build.mozilla.org/builddata/buildjson
"""
import json
import logging
import os
import requests

from mozci.utils.tzone import utc_dt, utc_time, utc_day

LOG = logging.getLogger()

BUILDJSON_DATA = "http://builddata.pub.build.mozilla.org/builddata/buildjson"
BUILDS_4HR_FILE = "builds-4hr.js.gz"
BUILDS_DAY_FILE = "builds-%s.js"


def _fetch_file(data_file, url):
    LOG.debug("We will now fetch %s" % url)
    # Fetch tar ball
    req = requests.get(url)
    # NOTE: requests deals with decrompressing the gzip file
    with open(data_file, 'wb') as fd:
        for chunk in req.iter_content(chunk_size=1024):
            fd.write(chunk)


def _fetch_buildjson_day_file(date):
    '''
       In BUILDJSON_DATA we have the information about all jobs stored
       as a gzip file per day.

       This function caches the uncompressed gzip files requested in the past.

       This function returns a json object containing all jobs for a given day.
    '''
    data_file = BUILDS_DAY_FILE % date

    if not os.path.exists(data_file):
        url = "%s/%s.gz" % (BUILDJSON_DATA, data_file)
        LOG.debug("We have not been able to find on disk %s." % data_file)
        _fetch_file(data_file, url)

    return json.load(open(data_file))["builds"]


def _fetch_buildjson_4hour_file():
    '''
    This file is generate every minute.
    It has the same data as today's buildjson day file but only for the
    last 4 hours.
    '''
    LOG.debug("Fetching %s..." % BUILDS_4HR_FILE)
    url = "%s/%s" % (BUILDJSON_DATA, BUILDS_4HR_FILE)
    _fetch_file(BUILDS_4HR_FILE, url)
    return json.load(open(BUILDS_4HR_FILE))["builds"]


def _find_job(request_id, builds, filename):
    '''
    Look for request_id in builds extracted from filename.

    raises Exception when we can't find the job.
    '''
    LOG.debug("We are going to look for %s in %s." % (request_id, filename))

    for job in builds:
        if request_id in job["request_ids"]:
            LOG.debug("Found %s" % str(job))
            return job

    raise Exception(
        "We have not found the job. If you see this problem please grep "
        "in %s for %d and run again with --debug and --dry-run." % (filename, request_id)
    )


def query_job_data(complete_at, request_id):
    """
    This function looks for a job identified by `request_id` inside of a
    buildjson file under the "builds" entry.

    Through `complete_at`, we can determine on which day we can find the
    metadata about this job.

    If found, the returning entry will look like this (only important values
    are referenced):

    .. code-block:: python

        {
            "builder_id": int, # It is a unique identifier of a builder
            "starttime": int,
            "endtime": int,
            "properties": {
                "buildername": string,
                "buildid": string,
                "revision": string,
                "repo_path": string, # e.g. projects/cedar
                "log_url", string,
                "slavename": string, # e.g. t-w864-ix-120
                "packageUrl": string, # It only applies for build jobs
                "testsUrl": string,   # It only applies for build jobs
                "blobber_files": json, # Mainly applicable to test jobs
                "symbolsUrl": string, # It only applies for build jobs
            },
            "request_ids": list of ints, # Scheduling ID
            "requestime": int,
            "result": int, # Job's exit code
            "slave_id": int, # Unique identifier for the machine that run it
        }

    NOTE: Remove this block once https://bugzilla.mozilla.org/show_bug.cgi?id=1135991
    is fixed.

    There is so funkiness in here. A buildjson file for a day is produced
    every 15 minutes all the way until midnight pacific time. After that, a new
    _UTC_ day comences. However, we will only contain all jobs ending within the
    UTC day and not the PT day. If you run any of this code in the last 4 hours of
    the pacific day, you will have a gap of 4 hours for which you won't have buildjson
    data (between 4-8pm PT). The gap starts appearing after 8pm PT when builds-4hr
    cannot cover it.

    If we look all endtime values on a day and we print the minimum and maximues values,
    this is what we get:

    .. code-block:: python

        1424649600 Mon, 23 Feb 2015 00:00:00  () Sun, 22 Feb 2015 16:00:00 -0800 (PST)
        1424736000 Tue, 24 Feb 2015 00:00:00  () Mon, 23 Feb 2015 16:00:00 -0800 (PST)

    This means that since 4pm to midnight we generate the same file again and again
    without adding any new data.
    """
    assert type(request_id) is int
    assert type(complete_at) is int

    date = utc_day(complete_at)
    LOG.debug("Job identified with complete_at value: %d run on %s UTC." % (complete_at, date))

    then = utc_dt(complete_at)
    hours_ago = (utc_dt() - then).total_seconds() / (60 * 60)
    LOG.debug("The job completed at %s (%d hours ago)." % (utc_time(complete_at), hours_ago))

    # If it has finished in the last 4 hours
    if hours_ago < 4:
        filename = BUILDS_4HR_FILE
        job = _find_job(request_id, _fetch_buildjson_4hour_file(), filename)
    else:
        filename = BUILDS_DAY_FILE % date
        builds = _fetch_buildjson_day_file(date)
        # If it is today's date we might need to clobber the file since we could
        # have cached today's file for a job earlier in the day
        if utc_day() == date and os.path.exists(filename):
            try:
                job = _find_job(request_id, builds, filename)
            except:
                last_modified = int(os.path.getmtime(filename)) / 60
                LOG.info("We removed today's buildjson file since the job was not found.")
                LOG.info("We will fetch again since it is %s minutes out of date." %
                         last_modified)
                os.remove(filename)
                builds = _fetch_buildjson_day_file(date)
                job = _find_job(request_id, builds, filename)
        else:
            job = _find_job(request_id, builds, filename)

    return job
