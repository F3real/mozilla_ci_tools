#!/usr/bin/env python
"""
This module helps with the buildjson data generated by the Release Engineering
systems: http://builddata.pub.build.mozilla.org/builddata/buildjson
"""
import json
import logging
import os
import requests
import time

from mozci.utils.tzone import utc_dt, utc_time, utc_day

LOG = logging.getLogger()

BUILDJSON_DATA = "http://builddata.pub.build.mozilla.org/builddata/buildjson"
BUILDS_4HR_FILE = "builds-4hr.js"
BUILDS_DAY_FILE = "builds-%s.js"

# This helps us read into memory and load less from disk
BUILDS_DAY_INDEX = {}


def _fetch_file(data_file, url):
    LOG.debug("We will now fetch %s" % url)
    # Fetch tar ball
    req = requests.get(url)
    # NOTE: requests deals with decompressing the gzip file
    with open(data_file, 'wb') as fd:
        for chunk in req.iter_content(chunk_size=1024):
            fd.write(chunk)


def _fetch_buildjson_day_file(date):
    """
    Return a json object containing all jobs for a given day.

    In BUILDJSON_DATA we have the information about all jobs stored
    as a gzip file per day.

    This function caches the uncompressed gzip files requested in the past.
    """
    data_file = BUILDS_DAY_FILE % date

    if not os.path.exists(data_file):
        url = "%s/%s.gz" % (BUILDJSON_DATA, data_file)
        LOG.debug("We have not been able to find on disk %s." % data_file)
        _fetch_file(data_file, url)

    return json.load(open(data_file))["builds"]


def _fetch_buildjson_4hour_file():
    """
    builds_4hr is generated every minute.

    It has the same data as today's buildjson day file but only for the
    last 4 hours.
    """
    LOG.debug("Fetching %s..." % BUILDS_4HR_FILE)
    url = "%s/%s.gz" % (BUILDJSON_DATA, BUILDS_4HR_FILE)
    LOG.debug("We always delete %s" % BUILDS_4HR_FILE)
    if os.path.exists(BUILDS_4HR_FILE):
        last_modified = int(os.path.getmtime(BUILDS_4HR_FILE))
        now = int(time.time())
        # If older than a minute; clobber
        if (now - last_modified) > 60:
            os.remove(BUILDS_4HR_FILE)
    _fetch_file(BUILDS_4HR_FILE, url)
    return json.load(open(BUILDS_4HR_FILE))["builds"]


def _find_job(request_id, jobs, loaded_from):
    """
    Look for request_id in a list of jobs.
    loaded_from is simply to indicate where those jobs were loaded from.
    """
    found = None
    LOG.debug("We are going to look for %s in %s." % (request_id, loaded_from))

    for job in jobs:
        prop_req_ids = job["properties"].get("request_ids", [])
        if request_id in prop_req_ids:
            LOG.debug("Found %s" % str(job))
            found = job
            return job

    return found


def query_job_data(complete_at, request_id):
    """
    Look for a job identified by `request_id` inside of a buildjson
    file under the "builds" entry.

    Through `complete_at`, we can determine on which day we can find the
    metadata about this job.

    raises Exception when we can't find the job.

    WARNING: "request_ids" and the ones from "properties" can differ. Issue filed.

    If found, the returning entry will look like this (only important values
    are referenced):

    .. code-block:: python

        {
            "builder_id": int, # It is a unique identifier of a builder
            "starttime": int,
            "endtime": int,
            "properties": {
                "blobber_files": json, # Mainly applicable to test jobs
                "buildername": string,
                "buildid": string,
                "log_url", string,
                "packageUrl": string, # It only applies for build jobs
                "revision": string,
                "repo_path": string, # e.g. projects/cedar
                "request_ids": list of ints, # Scheduling ID
                "slavename": string, # e.g. t-w864-ix-120
                "symbolsUrl": string, # It only applies for build jobs
                "testsUrl": string,   # It only applies for build jobs
            },
            "request_ids": list of ints, # Scheduling ID
            "requesttime": int,
            "result": int, # Job's exit code
            "slave_id": int, # Unique identifier for the machine that run it
        }

    NOTE: Remove this block once https://bugzilla.mozilla.org/show_bug.cgi?id=1135991
    is fixed.

    There is so funkiness in here. A buildjson file for a day is produced
    every 15 minutes all the way until midnight pacific time. After that, a new
    _UTC_ day commences. However, we will only contain all jobs ending within the
    UTC day and not the PT day. If you run any of this code in the last 4 hours of
    the pacific day, you will have a gap of 4 hours for which you won't have buildjson
    data (between 4-8pm PT). The gap starts appearing after 8pm PT when builds-4hr
    cannot cover it.

    If we look all endtime values on a day and we print the minimum and maximues values,
    this is what we get:

    .. code-block:: python

        1424649600 Mon, 23 Feb 2015 00:00:00  () Sun, 22 Feb 2015 16:00:00 -0800 (PST)
        1424736000 Tue, 24 Feb 2015 00:00:00  () Mon, 23 Feb 2015 16:00:00 -0800 (PST)

    This means that since 4pm to midnight we generate the same file again and again
    without adding any new data.
    """
    assert type(request_id) is int
    assert type(complete_at) is int

    global BUILDS_DAY_INDEX

    date = utc_day(complete_at)
    LOG.debug("Job identified with complete_at value: %d run on %s UTC." % (complete_at, date))

    then = utc_dt(complete_at)
    hours_ago = (utc_dt() - then).total_seconds() / (60 * 60)
    LOG.debug("The job completed at %s (%d hours ago)." % (utc_time(complete_at), hours_ago))

    # If it has finished in the last 4 hours
    if hours_ago < 4:
        filename = BUILDS_4HR_FILE
        job = _find_job(request_id, _fetch_buildjson_4hour_file(), filename)
    else:
        filename = BUILDS_DAY_FILE % date
        # If it is today's date we might need to clobber the file since we could
        # have cached today's file for a job earlier in the day
        if utc_day() == date and os.path.exists(filename):
            # TODO: We need to optimize this to use the in-memory index
            try:
                job = _find_job(request_id, _fetch_buildjson_day_file(date), filename)
            except:
                last_modified = int(os.path.getmtime(filename)) / 60
                LOG.info("We removed today's buildjson file since the job was not found.")
                LOG.info("We will fetch again since it is %s minutes out of date." %
                         last_modified)
                os.remove(filename)
                job = _find_job(request_id, _fetch_buildjson_day_file(date), filename)
        else:
            if date in BUILDS_DAY_INDEX:
                LOG.debug("%s is loaded on memory; reading from there." % date)
            else:
                jobs = _fetch_buildjson_day_file(date)
                # Let's load the jobs into memory
                BUILDS_DAY_INDEX[date] = jobs

            job = _find_job(request_id, BUILDS_DAY_INDEX[date], filename)

    if job:
        return job

    raise Exception(
        "We have not found the job. If you see this problem please grep "
        "in %s for %d and run again with --debug and --dry-run. If you report "
        "this issue please upload the mentioned file somewhere for "
        "inspection. Thanks!" % (filename, request_id)
    )
